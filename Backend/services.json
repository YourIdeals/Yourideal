[
  {
    "serviceId": "SV-0000045562-0001",
    "clientId": "0000045562",
    "reference": "YIL-NA-520",
    "serviceType": "Managed Account and Payroll",
    "setupFee": "Managed Account and Payroll Setup Cost",
    "setupBudget": 2700,
    "startDate": "2024-09-01",
    "endDate": "",
    "referredBy": "Adult",
    "insurance": "Fish Insurance",
    "monthlyFee": 37.5,
    "initialFee": 102,
    "pensionSetup": 22,
    "pensionFee": 27,
    "annualFee": 27,
    "yearEndFee": 22,
    "carerBudget": 236.7,
    "agencyBudget": 424.6,
    "carers": [
      "Anwar Basha",
      "Deep Mary Johnson"
    ],
    "agency": [
      "Bioluminuex LTD"
    ],
    "pa": [
      "Deborah Tokunboh"
    ],
    "optional": [],
    "notes": "Artificial Intelligence\nAudio Deep Learning Made Simple: Sound Classification, step-by-step\nAn end-to-end example and architecture for Audio Deep Learning's foundational application scenario, in Plain English.\nKetan Doshi\nMar 18, 2021\n14 min read\nHands-on Tutorials, INTUITIVE AUDIO DEEP LEARNING SERIES\nPhoto by bruce mars on Unsplash\nPhoto by bruce mars on Unsplash\n\nSound Classification is one of the most widely used applications in Audio Deep Learning. It involves learning to classify sounds and to predict the category of that sound. This type of problem can be applied to many practical scenarios e.g. classifying music clips to identify the genre of the music, or classifying short utterances by a set of speakers to identify the speaker based on the voice.\n\nIn this article, we will walk through a simple demo application so as to understand the approach used to solve such audio classification problems. My goal throughout will be to understand not just how something works but why it works that way.\n\nI have a few more articles in my audio deep learning series that you might find useful. They explore other fascinating topics in this space including how we prepare audio data for deep learning, why we use Mel Spectrograms for deep learning models and how they are generated and optimized.\n\n    State-of-the-Art Techniques (What is sound and how it is digitized. What problems is audio deep learning solving in our daily lives. What are Spectrograms and why they are all-important.)\n    Why Mel Spectrograms perform better **** (Processing audio data in Python. What are Mel Spectrograms and how to generate them)\n    Data Preparation and Augmentation **** (Enhance Spectrograms features for optimal performance by hyper-parameter tuning and data augmentation)\n    Automatic Speech Recognition (Speech-to-Text algorithm and architecture, using CTC Loss and Decoding for aligning sequences.)\n    Beam Search (Algorithm commonly used by Speech-to-Text and NLP applications to enhance predictions)\n\nAudio Classification\n\nJust like classifying hand-written digits using the MNIST dataset is considered a \u2018Hello World\"-type problem for Computer Vision, we can think of this application as the introductory problem for audio deep learning.\n\nWe will start with sound files, convert them into spectrograms, input them into a CNN plus Linear Classifier model, and produce predictions about the class to which the sound belongs.\nAudio Classification application (Image by Author)\nAudio Classification application (Image by Author)\n\nThere are many suitable datasets available for sounds of different types. These datasets contain a large number of audio samples, along with a class label for each sample that identifies what type of sound it is, based on the problem you are trying to address.\n\nThese class labels can often be obtained from some part of the filename of the audio sample or from the sub-folder name in which the file is located. Alternately the class labels are specified in a separate metadata file, usually in TXT, JSON, or CSV format.\nExample problem \u2013 Classifying ordinary city sounds\n\nFor our demo, we will use the Urban Sound 8K dataset that consists of a corpus of ordinary sounds recorded from day-to-day city life. The sounds are taken from 10 classes such as drilling, dogs barking, and sirens. Each sound sample is labelled with the class to which it belongs.\n\nAfter downloading the dataset, we see that it consists of two parts:\n\n    Audio files in the \u2018audio\u2018 folder: It has 10 sub-folders named \u2018fold1\u2018 through \u2018fold10\u2018. Each sub-folder contains a number of \u2018.wav\u2018 audio samples eg. \u2018fold1/103074\u20137\u20131\u20130.wav\u2018\n    Metadata in the \u2018metadata\u2019 folder: It has a file \u2018UrbanSound8K.csv\u2018 that contains information about each audio sample in the dataset such as its filename, its class label, the \u2018fold\u2019 sub-folder location, and so on. The class label is a numeric Class ID from 0\u20139 for each of the 10 classes. eg. the number 0 means air conditioner, 1 is a car horn, and so on.\n\nThe samples are around 4 seconds in length. Here\u2019s what one sample looks like:\nAn audio sample of a drill (Image by Author)\nAn audio sample of a drill (Image by Author)\nSample Rate, Number of Channels, Bits, and Audio Encoding\nSample Rate, Number of Channels, Bits, and Audio Encoding\n\nThe recommendation of the dataset creators is to use the folds for doing 10-fold cross-validation to report metrics and evaluate the performance of your model. However, since our goal in this article is primarily as a demo of an audio deep learning example rather than to obtain the best metrics, we will ignore the folds and treat all the samples simply as one large dataset.\nPrepare training data\n\nAs for most deep learning problems, we will follow these steps:\nDeep Learning Workflow (Image by Author)\nDeep Learning Workflow (Image by Author)\n\nThe training data for this problem will be fairly simple:\n\n    The features (X) are the audio file paths\n    The target labels (y) are the class names\n\nSince the dataset has a metadata file that contains this information already, we can use that directly. The metadata contains information about each audio file.\n\nSince it is a CSV file, we can use Pandas to read it. We can prepare the feature and label data from the metadata.\n\nThis gives us the information we need for our training data.\nTraining data with audio file paths and class IDs\nTraining data with audio file paths and class IDs\nScan the audio file directory when metadata isn\u2019t available\n\nHaving the metadata file made things easy for us. How would we prepare our data for datasets that do not contain a metadata file?\n\nMany datasets consist of only audio files arranged in a folder structure from which class labels can be derived. To prepare our training data in this format, we would do the following:\nPreparing Training Data when metadata isn't available (Image by Author)\nPreparing Training Data when metadata isn\u2019t available (Image by Author)\n\n    Scan the directory and prepare a list of all the audio file paths.\n    Extract the class label from each file name, or from the name of the parent sub-folder\n    Map each class name from text to a numeric class ID\n\nWith or without metadata, the result would be the same \u2013 features consisting of a list of audio file names and target labels consisting of class IDs.\nAudio Pre-processing: Define Transforms\n\nThis training data with audio file paths cannot be input directly into the model. We have to load the audio data from the file and process it so that it is in a format that the model expects.\n\nThis audio pre-processing will all be done dynamically at runtime when we will read and load the audio files. This approach is similar to what we would do with image files as well. Since audio data, like image data, can be fairly large and memory-intensive, we don\u2019t want to read the entire dataset into memory all at once, ahead of time. So we keep only the audio file names (or image file names) in our training data.\n\nThen, at runtime, as we train the model one batch at a time, we will load the audio data for that batch and process it by applying a series of transforms to the audio. That way we keep audio data for only one batch in memory at a time.\n\nWith image data, we might have a pipeline of transforms where we first read the image file as pixels and load it. Then we might apply some image processing steps to reshape and resize the data, crop them to a fixed size and convert them into grayscale from RGB. We might also apply some image augmentation steps like rotation, flips, and so on.\n\nThe processing for audio data is very similar. Right now we\u2019re only defining the functions, they will be run a little later when we feed data to the model during training.\nPre-processing the training data for input to our model (Image by Author)\nPre-processing the training data for input to our model (Image by Author)\nRead audio from a file\n\nThe first thing we need is to read and load the audio file in \".wav\" format. Since we are using Pytorch for this example, the implementation below uses torchaudio for the audio processing, but librosa will work just as well.\nSome of the sound files are mono (ie. 1 audio channel) while most of them are stereo (ie. 2 audio channels). Since our model expects all items to have the same dimensions, we will convert the mono files to stereo, by duplicating the first channel to the second.\nStandardize sampling rate\n\nSome of the sound files are sampled at a sample rate of 48000Hz, while most are sampled at a rate of 44100Hz. This means that 1 second of audio will have an array size of 48000 for some sound files, while it will have a smaller array size of 44100 for the others. Once again, we must standardize and convert all audio to the same sampling rate so that all arrays have the same dimensions.\n\nWe then resize all the audio samples to have the same length by either extending its duration by padding it with silence, or by truncating it. We add that method to our AudioUtil class.\n\nNext, we can do data augmentation on the raw audio signal by applying a Time Shift to shift the audio to the left or the right by a random amount. I go into a lot more detail about this and other data augmentation techniques in this article.\nTime Shift of the audio wave (Image by Author)\nTime Shift of the audio wave (Image by Author)\nWe then convert the augmented audio to a Mel Spectrogram. They capture the essential features of the audio and are often the most suitable way to input audio data into deep learning models. To get more background about this, you might want to read my articles ([here](https://towardsdatascience.com/audio-deep-learning-made-simple-part-3-data-preparation-and-augmentation-24c6e1f6b52) and here) which explain in simple words what a Mel Spectrogram is, why they are crucial for audio deep learning, as well as how they are generated and how to tune them for getting the best performance from your models.\n\nNow we can do another round of augmentation, this time on the Mel Spectrogram rather than on the raw audio. We will use a technique called SpecAugment that uses these two methods:\n\n    Frequency mask \u2013 randomly mask out a range of consecutive frequencies by adding horizontal bars on the spectrogram.\n Time mask \u2013 similar to frequency masks, except that we randomly block out ranges of time from the spectrogram by using vertical bars.\nDefine Custom Data Loader\nNow that we have defined all the pre-processing transform functions we will define a custom Pytorch Dataset object.\nTo feed your data to a model with Pytorch, we need two objects:\nA custom Dataset object that uses all the audio transforms to pre-process an audio file and prepares one data item at a time.\n\n    A built-in DataLoader object that uses the Dataset object to fetch individual data items and packages them into a batch of data.\n\nWhen we start training, the Data Loader will randomly fetch one batch of input Features containing the list of audio file names and run the pre-processing audio transforms on each audio file. It will also fetch a batch of the corresponding target Labels containing the class IDs. Thus it will output one batch of training data at a time, which can directly be fed as input to our deep learning model.\nData Loader applies transforms and prepares one batch of data at a time (Image by Author)\nData Loader applies transforms and prepares one batch of data at a time (Image by Author)\n\nLet\u2019s walk through the steps as our data gets transformed, starting with an audio file:\n\n    The audio from the file gets loaded into a Numpy array of shape (num_channels, num_samples). Most of the audio is sampled at 44.1kHz and is about 4 seconds in duration, resulting in 44,100 * 4 = 176,400 samples. If the audio has 1 channel, the shape of the array will be (1, 176,400). Similarly, audio of 4 seconds duration with 2 channels and sampled at 48kHz will have 192,000 samples and a shape of (2, 192,000).\n    Since the channels and sampling rates of each audio are different, the next two transforms resample the audio to a standard 44.1kHz and to a standard 2 channels.\n    Since some audio clips might be more or less than 4 seconds, we also standardize the audio duration to a fixed length of 4 seconds. Now arrays for all items have the same shape of (2, 176,400)\n    The Time Shift data augmentation now randomly shifts each audio sample forward or backward. The shapes are unchanged.\n    The augmented audio is now converted into a Mel Spectrogram, resulting in a shape of (num_channels, Mel freq_bands, time_steps) = (2, 64, 344)\n    The SpecAugment data augmentation now randomly applies Time and Frequency Masks to the Mel Spectrograms. The shapes are unchanged.\n\nThus, each batch will have two tensors, one for the X feature data containing the Mel Spectrograms and the other for the y target labels containing numeric Class IDs. The batches are picked randomly from the training data for each training epoch.\n\nEach batch has a shape of (batch_sz, num_channels, Mel freq_bands, time_steps)\nA batch of (X, y) data\nA batch of (X, y) data\n\nWe can visualize one item from the batch. We see the Mel Spectrogram with vertical and horizontal stripes showing the Frequency and Time Masking data augmentation.\n\nThe data is now ready for input to the model.\nCreate Model\n\nThe data processing steps that we just did are the most unique aspects of our audio classification problem. From here on, the model and training procedure are quite similar to what is commonly used in a standard image classification problem and are not specific to audio deep learning.\n\nSince our data now consists of Spectrogram images, we build a CNN classification architecture to process them. It has four convolutional blocks which generate the feature maps. That data is then reshaped into the format we need so it can be input into the linear classifier layer, which finally outputs the predictions for the 10 classes.\nThe model takes a batch of pre-processed data and outputs class predictions (Image by Author)\nThe model takes a batch of pre-processed data and outputs class predictions (Image by Author)\n\nA few more details about how the model processes a batch of data:\n\n    A batch of images is input to the model with shape (batch_sz, num_channels, Mel freq_bands, time_steps) ie. (16, 2, 64, 344).\n    Each CNN layer applies its filters to step up the image depth ie. number of channels. The image width and height are reduced as the kernels and strides are applied. Finally, after passing through the four CNN layers, we get the output feature maps ie. (16, 64, 4, 22).\n    This gets pooled and flattened to a shape of (16, 64) and then input to the Linear layer.\n\nLinear update"
  },
  {
    "serviceId": "SV-R0000045562-0001",
    "clientId": "R0000045562",
    "reference": "YIL-RB-001",
    "serviceType": "Managed Account Only",
    "setupFee": "Managed Account Setup Cost",
    "setupBudget": 2450,
    "startDate": "2025-11-01",
    "endDate": "",
    "referredBy": "Adult",
    "insurance": "Aviva Insurance",
    "monthlyFee": 25.45,
    "initialFee": 101.45,
    "pensionSetup": 0,
    "pensionFee": 0,
    "annualFee": 0,
    "yearEndFee": 0,
    "carerBudget": 0,
    "agencyBudget": 148.5,
    "carers": [],
    "agency": [
      "Bio"
    ],
    "pa": [
      "TDB"
    ],
    "optional": [],
    "notes": ""
  },
  {
    "serviceId": "SV-R0000045562-0002",
    "clientId": "R0000045562",
    "reference": "YIL-RB-001",
    "serviceType": "Payroll Only",
    "setupFee": "Payroll Setup Cost",
    "setupBudget": 2150,
    "startDate": "2026-01-01",
    "endDate": "",
    "referredBy": "Adult",
    "insurance": "Aviva",
    "monthlyFee": 24.5,
    "initialFee": 101.5,
    "pensionSetup": 22,
    "pensionFee": 22,
    "annualFee": 0,
    "yearEndFee": 27,
    "carerBudget": 147.85,
    "agencyBudget": 0,
    "carers": [
      "TDB"
    ],
    "agency": [],
    "pa": [
      "DB"
    ],
    "optional": [],
    "notes": ""
  }
]